{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsipBGHmyxPi"
      },
      "source": [
        "# Install necessary libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XtGW74OZyhAv",
        "outputId": "d56278c8-dd5e-4c1b-cee0-26eb9d88a80f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sat Jul 12 12:51:09 2025       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n",
            "|-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   56C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "                                                                                         \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.5/181.5 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m898.5/898.5 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m117.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m68.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m22.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m83.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCreating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi\n",
        "!pip install -q supervision \"ultralytics<=8.3.40\"\n",
        "import cv2\n",
        "\n",
        "import numpy as np\n",
        "import supervision as sv\n",
        "\n",
        "from tqdm import tqdm\n",
        "from ultralytics import YOLO\n",
        "from supervision.assets import VideoAssets, download_assets\n",
        "from collections import defaultdict, deque"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s5ipT_ndunSC",
        "outputId": "ccd8e2c3-af24-4f9d-878e-a68ec48504a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Video:  99%|█████████▉| 785/793 [01:41<00:01,  7.75it/s]\n"
          ]
        }
      ],
      "source": [
        "# cars only in polygone zone annotated\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "\n",
        "# Constants\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/IU of Applied Sciences - Master of Artificial Intelligence/Master Thesis with Mushyam, Aditya, Dr./CODES and archive/Speed Estimation Project/Data/IMG_3575.mov\"  # Update your path\n",
        "TARGET_VIDEO_PATH = \"vehicles-result.mp4\"\n",
        "CONFIDENCE_THRESHOLD = 0.3\n",
        "IOU_THRESHOLD = 0.5\n",
        "MODEL_NAME = \"yolov8s.pt\"\n",
        "MODEL_RESOLUTION = 1280\n",
        "\n",
        "# Source points for perspective transform\n",
        "SOURCE = np.array([\n",
        "    [845, 372],\n",
        "    [1435, 372],\n",
        "    [1789, 803],\n",
        "    [659, 803]\n",
        "])\n",
        "\n",
        "# Ground truth in meters\n",
        "TARGET_WIDTH = 7.5\n",
        "TARGET_HEIGHT = 20\n",
        "\n",
        "TARGET = np.array([\n",
        "    [0, 0],\n",
        "    [TARGET_WIDTH - 1, 0],\n",
        "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
        "    [0, TARGET_HEIGHT - 1],\n",
        "])\n",
        "\n",
        "class ViewTransformer:\n",
        "    def __init__(self, source: np.ndarray, target: np.ndarray):\n",
        "        source = source.astype(np.float32)\n",
        "        target = target.astype(np.float32)\n",
        "        self.m = cv2.getPerspectiveTransform(source, target)\n",
        "        self.m_inv = cv2.getPerspectiveTransform(target, source)  # For reverse transforms\n",
        "\n",
        "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
        "        if points.size == 0:\n",
        "            return points\n",
        "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\n",
        "        return transformed_points.reshape(-1, 2)\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
        "if not cap.isOpened():\n",
        "    raise ValueError(\"Could not open video file\")\n",
        "\n",
        "# Get video properties\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Initialize video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(TARGET_VIDEO_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "# Initialize YOLO model\n",
        "model = YOLO(MODEL_NAME)\n",
        "view_transformer = ViewTransformer(source=SOURCE, target=TARGET)\n",
        "\n",
        "# Tracking variables\n",
        "active_trackers = set()\n",
        "was_in_zone = {}\n",
        "counted_vehicles = set()\n",
        "exit_counter = 0\n",
        "last_vehicles = []\n",
        "all_speeds = []\n",
        "coordinates = defaultdict(lambda: deque(maxlen=fps * 2))  # Store coordinates for 2 seconds\n",
        "\n",
        "# Process video\n",
        "pbar = tqdm(total=total_frames, desc=\"Processing Video\")\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    pbar.update(1)\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    # Vehicle detection with tracking\n",
        "    results = model.track(\n",
        "        frame,\n",
        "        persist=True,\n",
        "        classes=[2, 5, 7],  # Car, bus, truck\n",
        "        conf=CONFIDENCE_THRESHOLD,\n",
        "        iou=IOU_THRESHOLD,\n",
        "        imgsz=MODEL_RESOLUTION,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Extract detections\n",
        "    detections = results[0].boxes\n",
        "    boxes = detections.xyxy.cpu().numpy()\n",
        "    tracker_ids = detections.id.cpu().numpy() if detections.id is not None else []\n",
        "    confidences = detections.conf.cpu().numpy()\n",
        "    class_ids = detections.cls.cpu().numpy()\n",
        "\n",
        "    # Check which detections are in the polygon zone\n",
        "    current_in_zone = []\n",
        "    points = []\n",
        "    for i, box in enumerate(boxes):\n",
        "        x1, y1, x2, y2 = box\n",
        "        bottom_center = np.array([[(x1 + x2) / 2, y2]])\n",
        "        in_zone = cv2.pointPolygonTest(SOURCE, (bottom_center[0, 0], bottom_center[0, 1]), False) >= 0\n",
        "        current_in_zone.append(in_zone)\n",
        "        points.append(bottom_center[0])\n",
        "\n",
        "    points = np.array(points)\n",
        "    world_points = view_transformer.transform_points(points).astype(int)\n",
        "\n",
        "    # Update tracking status and count exits\n",
        "    for i, tracker_id in enumerate(tracker_ids):\n",
        "        if current_in_zone[i]:\n",
        "            active_trackers.add(tracker_id)\n",
        "            was_in_zone[tracker_id] = True\n",
        "        elif tracker_id in active_trackers:\n",
        "            if was_in_zone.get(tracker_id, False) and tracker_id not in counted_vehicles:\n",
        "                exit_counter += 1\n",
        "                counted_vehicles.add(tracker_id)\n",
        "\n",
        "                # Calculate average speed when vehicle exits\n",
        "                if len(coordinates.get(tracker_id, [])) >= fps / 2:\n",
        "                    coordinate_start = coordinates[tracker_id][-1]\n",
        "                    coordinate_end = coordinates[tracker_id][0]\n",
        "                    distance = abs(coordinate_start - coordinate_end)\n",
        "                    time = len(coordinates[tracker_id]) / fps\n",
        "                    avg_speed = distance / time * 3.6\n",
        "                    all_speeds.append(avg_speed)\n",
        "                    last_vehicles.append((tracker_id, int(avg_speed)))\n",
        "                    if len(last_vehicles) > 5:\n",
        "                        last_vehicles.pop(0)\n",
        "\n",
        "            was_in_zone[tracker_id] = False\n",
        "\n",
        "    # Store detections position (only while in zone)\n",
        "    for i, tracker_id in enumerate(tracker_ids):\n",
        "        if current_in_zone[i]:\n",
        "            coordinates[tracker_id].append(world_points[i, 1])  # Store y-coordinate\n",
        "\n",
        "    # Annotate frame\n",
        "    for i, (box, tracker_id) in enumerate(zip(boxes, tracker_ids)):\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        conf = confidences[i]\n",
        "        cls_id = class_ids[i]\n",
        "\n",
        "        # Draw bounding box\n",
        "        color = (0, 255, 0)  # Green\n",
        "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "        # Prepare label\n",
        "        if not current_in_zone[i]:\n",
        "            label = f\"#{tracker_id}\"\n",
        "        elif len(coordinates.get(tracker_id, [])) < fps / 2:\n",
        "            label = f\"#{tracker_id}\"\n",
        "        else:\n",
        "            coordinate_start = coordinates[tracker_id][-1]\n",
        "            coordinate_end = coordinates[tracker_id][0]\n",
        "            distance = abs(coordinate_start - coordinate_end)\n",
        "            time = len(coordinates[tracker_id]) / fps\n",
        "            speed = distance / time * 3.6\n",
        "            label = f\"#{tracker_id} {int(speed)} km/h\"\n",
        "\n",
        "        # Draw label\n",
        "        (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
        "        cv2.rectangle(annotated_frame, (x1, y1 - text_height - 10), (x1 + text_width + 10, y1), color, -1)\n",
        "        cv2.putText(annotated_frame, label, (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
        "\n",
        "        # Draw trace\n",
        "        if tracker_id in coordinates and len(coordinates[tracker_id]) > 1:\n",
        "            trace_points = []\n",
        "            for y_coord in coordinates[tracker_id]:\n",
        "                # Transform back to image coordinates\n",
        "                world_point = np.array([[points[i, 0], y_coord]], dtype=np.float32)\n",
        "                img_point = cv2.perspectiveTransform(world_point.reshape(-1, 1, 2), view_transformer.m_inv)\n",
        "                trace_points.append((int(img_point[0, 0, 0]), int(img_point[0, 0, 1])))\n",
        "\n",
        "            for j in range(1, len(trace_points)):\n",
        "                cv2.line(annotated_frame, trace_points[j-1], trace_points[j], (0, 255, 255), 2)\n",
        "\n",
        "    # Draw polygon zone\n",
        "    cv2.polylines(annotated_frame, [SOURCE.astype(int)], True, (255, 0, 0), 2)\n",
        "\n",
        "    # Display counters\n",
        "    text_scale = 1.2\n",
        "    text_thickness = 2\n",
        "    text_color = (255, 255, 255)\n",
        "    text_bg_color = (0, 0, 255)\n",
        "    line_height = 40\n",
        "    margin = 30\n",
        "    start_y = margin + 20\n",
        "\n",
        "    # Calculate average speed\n",
        "    avg_all_speed = int(np.mean(all_speeds)) if all_speeds else 0\n",
        "    counter_text = f\"Vehicles: {exit_counter} | Avg Speed: {avg_all_speed} km/h\"\n",
        "\n",
        "    # Draw counter\n",
        "    (text_width, text_height), _ = cv2.getTextSize(counter_text, cv2.FONT_HERSHEY_SIMPLEX, text_scale, text_thickness)\n",
        "    text_x = width - text_width - margin\n",
        "    cv2.rectangle(annotated_frame, (text_x - 10, start_y - text_height - 10), (text_x + text_width + 10, start_y + 10), text_bg_color, -1)\n",
        "    cv2.putText(annotated_frame, counter_text, (text_x, start_y), cv2.FONT_HERSHEY_SIMPLEX, text_scale, text_color, text_thickness, cv2.LINE_AA)\n",
        "\n",
        "    # Draw last vehicles\n",
        "    start_y += line_height\n",
        "    for i, (vid, speed) in enumerate(reversed(last_vehicles)):\n",
        "        vehicle_text = f\"#{vid}: {speed} km/h\"\n",
        "        (v_text_width, v_text_height), _ = cv2.getTextSize(vehicle_text, cv2.FONT_HERSHEY_SIMPLEX, text_scale*0.8, text_thickness-1)\n",
        "        v_text_x = width - v_text_width - margin\n",
        "        cv2.rectangle(annotated_frame, (v_text_x - 10, start_y - v_text_height - 5), (v_text_x + v_text_width + 10, start_y + 5), (50, 50, 150), -1)\n",
        "        cv2.putText(annotated_frame, vehicle_text, (v_text_x, start_y), cv2.FONT_HERSHEY_SIMPLEX, text_scale*0.8, text_color, text_thickness-1, cv2.LINE_AA)\n",
        "        start_y += line_height\n",
        "        if i >= 4: break\n",
        "\n",
        "    # Write frame\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "pbar.close()\n",
        "cap.release()\n",
        "out.release()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# all cars annotated\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Constants\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/IU of Applied Sciences - Master of Artificial Intelligence/Master Thesis with Mushyam, Aditya, Dr./CODES and archive/Speed Estimation Project/Data/IMG_3575.mov\"  # Update your path\n",
        "TARGET_VIDEO_PATH = \"vehicles-result.mp4\"\n",
        "CONFIDENCE_THRESHOLD = 0.3\n",
        "IOU_THRESHOLD = 0.5\n",
        "MODEL_NAME = \"yolov8s.pt\"\n",
        "MODEL_RESOLUTION = 1280\n",
        "\n",
        "# Source points for perspective transform\n",
        "SOURCE = np.array([\n",
        "    [845, 372],\n",
        "    [1435, 372],\n",
        "    [1789, 803],\n",
        "    [659, 803]\n",
        "])\n",
        "\n",
        "# Ground truth in meters\n",
        "TARGET_WIDTH = 7.5\n",
        "TARGET_HEIGHT = 20\n",
        "\n",
        "TARGET = np.array([\n",
        "    [0, 0],\n",
        "    [TARGET_WIDTH - 1, 0],\n",
        "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
        "    [0, TARGET_HEIGHT - 1],\n",
        "])\n",
        "\n",
        "class ViewTransformer:\n",
        "    def __init__(self, source: np.ndarray, target: np.ndarray):\n",
        "        source = source.astype(np.float32)\n",
        "        target = target.astype(np.float32)\n",
        "        self.m = cv2.getPerspectiveTransform(source, target)\n",
        "        self.m_inv = cv2.getPerspectiveTransform(target, source)  # For reverse transforms\n",
        "\n",
        "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
        "        if points.size == 0:\n",
        "            return points\n",
        "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\n",
        "        return transformed_points.reshape(-1, 2)\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
        "if not cap.isOpened():\n",
        "    raise ValueError(\"Could not open video file\")\n",
        "\n",
        "# Get video properties\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Initialize video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(TARGET_VIDEO_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "# Initialize YOLO model\n",
        "model = YOLO(MODEL_NAME)\n",
        "view_transformer = ViewTransformer(source=SOURCE, target=TARGET)\n",
        "\n",
        "# Tracking variables\n",
        "active_trackers = set()\n",
        "was_in_zone = {}\n",
        "counted_vehicles = set()\n",
        "exit_counter = 0\n",
        "last_vehicles = []\n",
        "all_speeds = []\n",
        "coordinates = defaultdict(lambda: deque(maxlen=fps * 2))  # Store coordinates for 2 seconds\n",
        "\n",
        "# Process video\n",
        "pbar = tqdm(total=total_frames, desc=\"Processing Video\")\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    pbar.update(1)\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    # Vehicle detection with tracking\n",
        "    results = model.track(\n",
        "        frame,\n",
        "        persist=True,\n",
        "        classes=[2, 5, 7],  # Car, bus, truck\n",
        "        conf=CONFIDENCE_THRESHOLD,\n",
        "        iou=IOU_THRESHOLD,\n",
        "        imgsz=MODEL_RESOLUTION,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Extract detections\n",
        "    detections = results[0].boxes\n",
        "    boxes = detections.xyxy.cpu().numpy()\n",
        "    tracker_ids = detections.id.cpu().numpy() if detections.id is not None else []\n",
        "    confidences = detections.conf.cpu().numpy()\n",
        "    class_ids = detections.cls.cpu().numpy()\n",
        "\n",
        "    # Check which detections are in the polygon zone\n",
        "    current_in_zone = []\n",
        "    points = []\n",
        "    for i, box in enumerate(boxes):\n",
        "        x1, y1, x2, y2 = box\n",
        "        bottom_center = np.array([[(x1 + x2) / 2, y2]])\n",
        "        in_zone = cv2.pointPolygonTest(SOURCE, (bottom_center[0, 0], bottom_center[0, 1]), False) >= 0\n",
        "        current_in_zone.append(in_zone)\n",
        "        points.append(bottom_center[0])\n",
        "\n",
        "    points = np.array(points)\n",
        "    world_points = view_transformer.transform_points(points).astype(int)\n",
        "\n",
        "    # Update tracking status and count exits\n",
        "    for i, tracker_id in enumerate(tracker_ids):\n",
        "        if current_in_zone[i]:\n",
        "            active_trackers.add(tracker_id)\n",
        "            was_in_zone[tracker_id] = True\n",
        "        elif tracker_id in active_trackers:\n",
        "            if was_in_zone.get(tracker_id, False) and tracker_id not in counted_vehicles:\n",
        "                exit_counter += 1\n",
        "                counted_vehicles.add(tracker_id)\n",
        "\n",
        "                # Calculate average speed when vehicle exits\n",
        "                if len(coordinates.get(tracker_id, [])) >= fps / 2:\n",
        "                    coordinate_start = coordinates[tracker_id][-1]\n",
        "                    coordinate_end = coordinates[tracker_id][0]\n",
        "                    distance = abs(coordinate_start - coordinate_end)\n",
        "                    time = len(coordinates[tracker_id]) / fps\n",
        "                    avg_speed = distance / time * 3.6\n",
        "                    all_speeds.append(avg_speed)\n",
        "                    last_vehicles.append((tracker_id, int(avg_speed)))\n",
        "                    if len(last_vehicles) > 5:\n",
        "                        last_vehicles.pop(0)\n",
        "\n",
        "            was_in_zone[tracker_id] = False\n",
        "\n",
        "    # Store detections position (only while in zone)\n",
        "    for i, tracker_id in enumerate(tracker_ids):\n",
        "        if current_in_zone[i]:\n",
        "            coordinates[tracker_id].append(world_points[i, 1])  # Store y-coordinate\n",
        "\n",
        "    # Annotate frame\n",
        "    for i, (box, tracker_id) in enumerate(zip(boxes, tracker_ids)):\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        conf = confidences[i]\n",
        "        cls_id = class_ids[i]\n",
        "\n",
        "        # Draw bounding box\n",
        "        color = (0, 255, 0)  # Green\n",
        "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "        # Prepare label\n",
        "        if not current_in_zone[i]:\n",
        "            label = f\"#{tracker_id}\"\n",
        "        elif len(coordinates.get(tracker_id, [])) < fps / 2:\n",
        "            label = f\"#{tracker_id}\"\n",
        "        else:\n",
        "            coordinate_start = coordinates[tracker_id][-1]\n",
        "            coordinate_end = coordinates[tracker_id][0]\n",
        "            distance = abs(coordinate_start - coordinate_end)\n",
        "            time = len(coordinates[tracker_id]) / fps\n",
        "            speed = distance / time * 3.6\n",
        "            label = f\"#{tracker_id} {int(speed)} km/h\"\n",
        "\n",
        "        # Draw label\n",
        "        (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
        "        cv2.rectangle(annotated_frame, (x1, y1 - text_height - 10), (x1 + text_width + 10, y1), color, -1)\n",
        "        cv2.putText(annotated_frame, label, (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
        "\n",
        "        # Draw trace\n",
        "        if tracker_id in coordinates and len(coordinates[tracker_id]) > 1:\n",
        "            trace_points = []\n",
        "            for y_coord in coordinates[tracker_id]:\n",
        "                # Transform back to image coordinates\n",
        "                world_point = np.array([[points[i, 0], y_coord]], dtype=np.float32)\n",
        "                img_point = cv2.perspectiveTransform(world_point.reshape(-1, 1, 2), view_transformer.m_inv)\n",
        "                trace_points.append((int(img_point[0, 0, 0]), int(img_point[0, 0, 1])))\n",
        "\n",
        "            for j in range(1, len(trace_points)):\n",
        "                cv2.line(annotated_frame, trace_points[j-1], trace_points[j], (0, 255, 255), 2)\n",
        "\n",
        "    # Draw polygon zone with dimensions\n",
        "    cv2.polylines(annotated_frame, [SOURCE.astype(int)], True, (255, 0, 0), 2)\n",
        "\n",
        "    # Add width and height annotations near the polygon\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 0.7\n",
        "    font_thickness = 2\n",
        "    text_color = (255, 255, 255)\n",
        "    bg_color = (0, 0, 255)\n",
        "\n",
        "    # Calculate positions for the text\n",
        "# Calculate positions for the text\n",
        "    top_center = (int((SOURCE[0][0] + SOURCE[1][0]) / 2), int((SOURCE[0][1] + SOURCE[1][1]) / 2))\n",
        "    left_center = (int((SOURCE[0][0] + SOURCE[3][0]) / 2), int((SOURCE[0][1] + SOURCE[3][1]) / 2))\n",
        "    right_center = (int((SOURCE[1][0] + SOURCE[2][0]) / 2), int((SOURCE[1][1] + SOURCE[2][1]) / 2))\n",
        "    bottom_center = (int((SOURCE[2][0] + SOURCE[3][0]) / 2), int((SOURCE[2][1] + SOURCE[3][1]) / 2))\n",
        "\n",
        "    # Draw width annotation (top side)\n",
        "    width_text = f\"{TARGET_WIDTH}m\"\n",
        "    (wt_width, wt_height), _ = cv2.getTextSize(width_text, font, font_scale, font_thickness)\n",
        "    cv2.rectangle(annotated_frame,\n",
        "                 (top_center[0] - wt_width//2 - 5, top_center[1] - wt_height - 5),\n",
        "                 (top_center[0] + wt_width//2 + 5, top_center[1] + 5),\n",
        "                 bg_color, -1)\n",
        "    cv2.putText(annotated_frame, width_text,\n",
        "               (top_center[0] - wt_width//2, top_center[1]),\n",
        "               font, font_scale, text_color, font_thickness)\n",
        "\n",
        "    # Draw height annotation (left side)\n",
        "    height_text = f\"{TARGET_HEIGHT}m\"\n",
        "    (ht_width, ht_height), _ = cv2.getTextSize(height_text, font, font_scale, font_thickness)\n",
        "    cv2.rectangle(annotated_frame,\n",
        "                 (left_center[0] - ht_width - 10, left_center[1] - ht_height//2),\n",
        "                 (left_center[0] - 5, left_center[1] + ht_height//2 + 5),\n",
        "                 bg_color, -1)\n",
        "    cv2.putText(annotated_frame, height_text,\n",
        "               (left_center[0] - ht_width - 5, left_center[1] + ht_height//2),\n",
        "               font, font_scale, text_color, font_thickness)\n",
        "\n",
        "    # Display counters\n",
        "    text_scale = 1.2\n",
        "    text_thickness = 2\n",
        "    text_color = (255, 255, 255)\n",
        "    text_bg_color = (0, 0, 255)\n",
        "    line_height = 40\n",
        "    margin = 30\n",
        "    start_y = margin + 20\n",
        "\n",
        "    # Calculate average speed\n",
        "    avg_all_speed = int(np.mean(all_speeds)) if all_speeds else 0\n",
        "    counter_text = f\"Vehicles: {exit_counter} | Avg Speed: {avg_all_speed} km/h\"\n",
        "\n",
        "    # Draw counter\n",
        "    (text_width, text_height), _ = cv2.getTextSize(counter_text, cv2.FONT_HERSHEY_SIMPLEX, text_scale, text_thickness)\n",
        "    text_x = width - text_width - margin\n",
        "    cv2.rectangle(annotated_frame, (text_x - 10, start_y - text_height - 10), (text_x + text_width + 10, start_y + 10), text_bg_color, -1)\n",
        "    cv2.putText(annotated_frame, counter_text, (text_x, start_y), cv2.FONT_HERSHEY_SIMPLEX, text_scale, text_color, text_thickness, cv2.LINE_AA)\n",
        "\n",
        "    # Draw last vehicles\n",
        "    start_y += line_height\n",
        "    for i, (vid, speed) in enumerate(reversed(last_vehicles)):\n",
        "        vehicle_text = f\"#{vid}: {speed} km/h\"\n",
        "        (v_text_width, v_text_height), _ = cv2.getTextSize(vehicle_text, cv2.FONT_HERSHEY_SIMPLEX, text_scale*0.8, text_thickness-1)\n",
        "        v_text_x = width - v_text_width - margin\n",
        "        cv2.rectangle(annotated_frame, (v_text_x - 10, start_y - v_text_height - 5), (v_text_x + v_text_width + 10, start_y + 5), (50, 50, 150), -1)\n",
        "        cv2.putText(annotated_frame, vehicle_text, (v_text_x, start_y), cv2.FONT_HERSHEY_SIMPLEX, text_scale*0.8, text_color, text_thickness-1, cv2.LINE_AA)\n",
        "        start_y += line_height\n",
        "        if i >= 4: break\n",
        "\n",
        "    # Write frame\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "pbar.close()\n",
        "cap.release()\n",
        "out.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACpezq5WvNUb",
        "outputId": "43c8d5cf-d0b0-400e-d232-481dbd02f70f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Video:  99%|█████████▉| 785/793 [01:37<00:00,  8.02it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#first 30 seconds of the video\n",
        "import cv2\n",
        "import numpy as np\n",
        "from collections import defaultdict, deque\n",
        "from ultralytics import YOLO\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Constants\n",
        "SOURCE_VIDEO_PATH = \"/content/drive/MyDrive/Arxiv and others/Project/New/IMG_3575.MOV\"  # Update your path\n",
        "\n",
        "TARGET_VIDEO_PATH = \"vehicles-result.mp4\"\n",
        "CONFIDENCE_THRESHOLD = 0.3\n",
        "IOU_THRESHOLD = 0.5\n",
        "MODEL_NAME = \"yolov8s.pt\"\n",
        "MODEL_RESOLUTION = 1280\n",
        "\n",
        "# Source points for perspective transform\n",
        "SOURCE = np.array([\n",
        "    [845, 372],\n",
        "    [1435, 372],\n",
        "    [1789, 803],\n",
        "    [659, 803]\n",
        "])\n",
        "\n",
        "# Ground truth in meters\n",
        "TARGET_WIDTH = 7.5\n",
        "TARGET_HEIGHT = 20\n",
        "\n",
        "TARGET = np.array([\n",
        "    [0, 0],\n",
        "    [TARGET_WIDTH - 1, 0],\n",
        "    [TARGET_WIDTH - 1, TARGET_HEIGHT - 1],\n",
        "    [0, TARGET_HEIGHT - 1],\n",
        "])\n",
        "\n",
        "class ViewTransformer:\n",
        "    def __init__(self, source: np.ndarray, target: np.ndarray):\n",
        "        source = source.astype(np.float32)\n",
        "        target = target.astype(np.float32)\n",
        "        self.m = cv2.getPerspectiveTransform(source, target)\n",
        "        self.m_inv = cv2.getPerspectiveTransform(target, source)  # For reverse transforms\n",
        "\n",
        "    def transform_points(self, points: np.ndarray) -> np.ndarray:\n",
        "        if points.size == 0:\n",
        "            return points\n",
        "        reshaped_points = points.reshape(-1, 1, 2).astype(np.float32)\n",
        "        transformed_points = cv2.perspectiveTransform(reshaped_points, self.m)\n",
        "        return transformed_points.reshape(-1, 2)\n",
        "\n",
        "# Initialize video capture\n",
        "cap = cv2.VideoCapture(SOURCE_VIDEO_PATH)\n",
        "if not cap.isOpened():\n",
        "    raise ValueError(\"Could not open video file\")\n",
        "\n",
        "# Get video properties\n",
        "fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
        "\n",
        "frames_to_process = fps * 30  # 30 seconds worth of frames\n",
        "\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
        "\n",
        "# Initialize video writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(TARGET_VIDEO_PATH, fourcc, fps, (width, height))\n",
        "\n",
        "# Initialize YOLO model\n",
        "model = YOLO(MODEL_NAME)\n",
        "view_transformer = ViewTransformer(source=SOURCE, target=TARGET)\n",
        "\n",
        "# Tracking variables\n",
        "active_trackers = set()\n",
        "was_in_zone = {}\n",
        "counted_vehicles = set()\n",
        "exit_counter = 0\n",
        "last_vehicles = []\n",
        "all_speeds = []\n",
        "coordinates = defaultdict(lambda: deque(maxlen=fps * 2))  # Store coordinates for 2 seconds\n",
        "\n",
        "# Process video\n",
        "pbar = tqdm(total=total_frames, desc=\"Processing Video\")\n",
        "\n",
        "frame_count = 0\n",
        "while cap.isOpened() and frame_count < frames_to_process:\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    frame_count += 1\n",
        "\n",
        "    pbar.update(1)\n",
        "    annotated_frame = frame.copy()\n",
        "\n",
        "    # Vehicle detection with tracking\n",
        "    results = model.track(\n",
        "        frame,\n",
        "        persist=True,\n",
        "        classes=[2, 5, 7],  # Car, bus, truck\n",
        "        conf=CONFIDENCE_THRESHOLD,\n",
        "        iou=IOU_THRESHOLD,\n",
        "        imgsz=MODEL_RESOLUTION,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    # Extract detections\n",
        "    detections = results[0].boxes\n",
        "    boxes = detections.xyxy.cpu().numpy()\n",
        "    tracker_ids = detections.id.cpu().numpy() if detections.id is not None else []\n",
        "    confidences = detections.conf.cpu().numpy()\n",
        "    class_ids = detections.cls.cpu().numpy()\n",
        "\n",
        "    # Check which detections are in the polygon zone\n",
        "    current_in_zone = []\n",
        "    points = []\n",
        "    for i, box in enumerate(boxes):\n",
        "        x1, y1, x2, y2 = box\n",
        "        bottom_center = np.array([[(x1 + x2) / 2, y2]])\n",
        "        in_zone = cv2.pointPolygonTest(SOURCE, (bottom_center[0, 0], bottom_center[0, 1]), False) >= 0\n",
        "        current_in_zone.append(in_zone)\n",
        "        points.append(bottom_center[0])\n",
        "\n",
        "    points = np.array(points)\n",
        "    world_points = view_transformer.transform_points(points).astype(int)\n",
        "\n",
        "    # Update tracking status and count exits\n",
        "    for i, tracker_id in enumerate(tracker_ids):\n",
        "        if current_in_zone[i]:\n",
        "            active_trackers.add(tracker_id)\n",
        "            was_in_zone[tracker_id] = True\n",
        "        elif tracker_id in active_trackers:\n",
        "            if was_in_zone.get(tracker_id, False) and tracker_id not in counted_vehicles:\n",
        "                exit_counter += 1\n",
        "                counted_vehicles.add(tracker_id)\n",
        "\n",
        "                # Calculate average speed when vehicle exits\n",
        "                if len(coordinates.get(tracker_id, [])) >= fps / 2:\n",
        "                    coordinate_start = coordinates[tracker_id][-1]\n",
        "                    coordinate_end = coordinates[tracker_id][0]\n",
        "                    distance = abs(coordinate_start - coordinate_end)\n",
        "                    time = len(coordinates[tracker_id]) / fps\n",
        "                    avg_speed = distance / time * 3.6\n",
        "                    all_speeds.append(avg_speed)\n",
        "                    last_vehicles.append((tracker_id, int(avg_speed)))\n",
        "                    if len(last_vehicles) > 5:\n",
        "                        last_vehicles.pop(0)\n",
        "\n",
        "            was_in_zone[tracker_id] = False\n",
        "\n",
        "    # Store detections position (only while in zone)\n",
        "    for i, tracker_id in enumerate(tracker_ids):\n",
        "        if current_in_zone[i]:\n",
        "            coordinates[tracker_id].append(world_points[i, 1])  # Store y-coordinate\n",
        "\n",
        "    # Annotate frame\n",
        "    for i, (box, tracker_id) in enumerate(zip(boxes, tracker_ids)):\n",
        "        x1, y1, x2, y2 = map(int, box)\n",
        "        conf = confidences[i]\n",
        "        cls_id = class_ids[i]\n",
        "\n",
        "        # Draw bounding box\n",
        "        color = (0, 255, 0)  # Green\n",
        "        cv2.rectangle(annotated_frame, (x1, y1), (x2, y2), color, 2)\n",
        "\n",
        "        # Prepare label\n",
        "        if not current_in_zone[i]:\n",
        "            label = f\"#{tracker_id}\"\n",
        "        elif len(coordinates.get(tracker_id, [])) < fps / 2:\n",
        "            label = f\"#{tracker_id}\"\n",
        "        else:\n",
        "            coordinate_start = coordinates[tracker_id][-1]\n",
        "            coordinate_end = coordinates[tracker_id][0]\n",
        "            distance = abs(coordinate_start - coordinate_end)\n",
        "            time = len(coordinates[tracker_id]) / fps\n",
        "            speed = distance / time * 3.6\n",
        "            label = f\"#{tracker_id} {int(speed)} km/h\"\n",
        "\n",
        "        # Draw label\n",
        "        (text_width, text_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.6, 1)\n",
        "        cv2.rectangle(annotated_frame, (x1, y1 - text_height - 10), (x1 + text_width + 10, y1), color, -1)\n",
        "        cv2.putText(annotated_frame, label, (x1 + 5, y1 - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 0), 1)\n",
        "\n",
        "        # Draw trace\n",
        "        if tracker_id in coordinates and len(coordinates[tracker_id]) > 1:\n",
        "            trace_points = []\n",
        "            for y_coord in coordinates[tracker_id]:\n",
        "                # Transform back to image coordinates\n",
        "                world_point = np.array([[points[i, 0], y_coord]], dtype=np.float32)\n",
        "                img_point = cv2.perspectiveTransform(world_point.reshape(-1, 1, 2), view_transformer.m_inv)\n",
        "                trace_points.append((int(img_point[0, 0, 0]), int(img_point[0, 0, 1])))\n",
        "\n",
        "            for j in range(1, len(trace_points)):\n",
        "                cv2.line(annotated_frame, trace_points[j-1], trace_points[j], (0, 255, 255), 2)\n",
        "\n",
        "    # Draw polygon zone with dimensions\n",
        "    cv2.polylines(annotated_frame, [SOURCE.astype(int)], True, (255, 0, 0), 2)\n",
        "\n",
        "    # Add width and height annotations near the polygon\n",
        "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
        "    font_scale = 0.7\n",
        "    font_thickness = 2\n",
        "    text_color = (255, 255, 255)\n",
        "    bg_color = (0, 0, 255)\n",
        "\n",
        "    # Calculate positions for the text\n",
        "# Calculate positions for the text\n",
        "    top_center = (int((SOURCE[0][0] + SOURCE[1][0]) / 2), int((SOURCE[0][1] + SOURCE[1][1]) / 2))\n",
        "    left_center = (int((SOURCE[0][0] + SOURCE[3][0]) / 2), int((SOURCE[0][1] + SOURCE[3][1]) / 2))\n",
        "    right_center = (int((SOURCE[1][0] + SOURCE[2][0]) / 2), int((SOURCE[1][1] + SOURCE[2][1]) / 2))\n",
        "    bottom_center = (int((SOURCE[2][0] + SOURCE[3][0]) / 2), int((SOURCE[2][1] + SOURCE[3][1]) / 2))\n",
        "\n",
        "    # Draw width annotation (top side)\n",
        "    width_text = f\"{TARGET_WIDTH}m\"\n",
        "    (wt_width, wt_height), _ = cv2.getTextSize(width_text, font, font_scale, font_thickness)\n",
        "    cv2.rectangle(annotated_frame,\n",
        "                 (top_center[0] - wt_width//2 - 5, top_center[1] - wt_height - 5),\n",
        "                 (top_center[0] + wt_width//2 + 5, top_center[1] + 5),\n",
        "                 bg_color, -1)\n",
        "    cv2.putText(annotated_frame, width_text,\n",
        "               (top_center[0] - wt_width//2, top_center[1]),\n",
        "               font, font_scale, text_color, font_thickness)\n",
        "\n",
        "    # Draw height annotation (left side)\n",
        "    height_text = f\"{TARGET_HEIGHT}m\"\n",
        "    (ht_width, ht_height), _ = cv2.getTextSize(height_text, font, font_scale, font_thickness)\n",
        "    cv2.rectangle(annotated_frame,\n",
        "                 (left_center[0] - ht_width - 10, left_center[1] - ht_height//2),\n",
        "                 (left_center[0] - 5, left_center[1] + ht_height//2 + 5),\n",
        "                 bg_color, -1)\n",
        "    cv2.putText(annotated_frame, height_text,\n",
        "               (left_center[0] - ht_width - 5, left_center[1] + ht_height//2),\n",
        "               font, font_scale, text_color, font_thickness)\n",
        "\n",
        "    # Display counters\n",
        "    text_scale = 1.2\n",
        "    text_thickness = 2\n",
        "    text_color = (255, 255, 255)\n",
        "    text_bg_color = (0, 0, 255)\n",
        "    line_height = 40\n",
        "    margin = 30\n",
        "    start_y = margin + 20\n",
        "\n",
        "    # Calculate average speed\n",
        "    avg_all_speed = int(np.mean(all_speeds)) if all_speeds else 0\n",
        "    counter_text = f\"Vehicles: {exit_counter} | Avg Speed: {avg_all_speed} km/h\"\n",
        "\n",
        "    # Draw counter\n",
        "    (text_width, text_height), _ = cv2.getTextSize(counter_text, cv2.FONT_HERSHEY_SIMPLEX, text_scale, text_thickness)\n",
        "    text_x = width - text_width - margin\n",
        "    cv2.rectangle(annotated_frame, (text_x - 10, start_y - text_height - 10), (text_x + text_width + 10, start_y + 10), text_bg_color, -1)\n",
        "    cv2.putText(annotated_frame, counter_text, (text_x, start_y), cv2.FONT_HERSHEY_SIMPLEX, text_scale, text_color, text_thickness, cv2.LINE_AA)\n",
        "\n",
        "    # Draw last vehicles\n",
        "    start_y += line_height\n",
        "    for i, (vid, speed) in enumerate(reversed(last_vehicles)):\n",
        "        vehicle_text = f\"#{vid}: {speed} km/h\"\n",
        "        (v_text_width, v_text_height), _ = cv2.getTextSize(vehicle_text, cv2.FONT_HERSHEY_SIMPLEX, text_scale*0.8, text_thickness-1)\n",
        "        v_text_x = width - v_text_width - margin\n",
        "        cv2.rectangle(annotated_frame, (v_text_x - 10, start_y - v_text_height - 5), (v_text_x + v_text_width + 10, start_y + 5), (50, 50, 150), -1)\n",
        "        cv2.putText(annotated_frame, vehicle_text, (v_text_x, start_y), cv2.FONT_HERSHEY_SIMPLEX, text_scale*0.8, text_color, text_thickness-1, cv2.LINE_AA)\n",
        "        start_y += line_height\n",
        "        if i >= 4: break\n",
        "\n",
        "    # Write frame\n",
        "    out.write(annotated_frame)\n",
        "\n",
        "pbar.close()\n",
        "cap.release()\n",
        "out.release()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQ2DV9sEvNRl",
        "outputId": "788110ea-566e-4703-9e02-f392cdb953b6"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Processing Video:  11%|█▏        | 870/7700 [01:47<14:02,  8.11it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m3RAM7O9vNNg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-yd-ZX8OvNJO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}